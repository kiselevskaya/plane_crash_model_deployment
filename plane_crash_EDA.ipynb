{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Data Analysis\n",
    "\n",
    "Implementation of each of the steps in the Machine Learning Pipeline. \n",
    "\n",
    "1. **Data Analysis**\n",
    "2. Feature Engineering\n",
    "3. Feature Selection\n",
    "4. Model Training\n",
    "5. Obtaining Predictions / Scoring\n",
    "\n",
    "Plane Crash Dataset available on [Kaggle.com](https://www.kaggle.com/datasets/kamilkarczmarczyk/plane-crash-dataset-03042023). See below for more details.\n",
    "\n",
    "==================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data description:\n",
    "- Date: Date of accident, in the format - January 01, 2001\n",
    "- Time: Local time, in 24 hr. format unless otherwise specified\n",
    "- Airline/Op: Airline or operator of the aircraft\n",
    "- Flight #: Flight number assigned by the aircraft operator\n",
    "- Route: Complete or partial route flown prior to the accident\n",
    "- AC Type: Aircraft type\n",
    "- Reg: ICAO registration of the aircraft\n",
    "- cn / ln: Construction or serial number / Line or fuselage number\n",
    "- Aboard: Total aboard (passengers / crew)\n",
    "- Fatalities: Total fatalities aboard (passengers / crew)\n",
    "- Ground: Total killed on the ground\n",
    "- Summary: Brief description of the accident and cause if known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# for the yeo-johnson transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "# to display all the columns of the dataframe in the notebook\n",
    "pd.pandas.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "from typing import Union\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import spacy.cli\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "raw_data = pd.read_csv(\"data/raw_data.csv\", sep=\";\")\n",
    "\n",
    "# rows and columns of the data\n",
    "print(raw_data.shape)\n",
    "\n",
    "# visualise the dataset\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSchema:\n",
    "    DATE = \"Date\"\n",
    "    TIME = \"Time:\"\n",
    "    LOCATION = \"Location:\"\n",
    "    AC_TYPE = \"AC        Type:\"\n",
    "    OPERATOR = \"Operator:\"\n",
    "    ROUTE = \"Route:\"\n",
    "    CN_LN = \"cn / ln:\"\n",
    "    FLIGHT_N = \"Flight #:\"\n",
    "    IS_MILITARY = \"Is_military\"\n",
    "    MILITARY_COUNTRY = \"Military country\"\n",
    "    ABOARD_ALL = \"Aboard_all\"\n",
    "    ABOARD_PASSENGERS = \"Aboard_passengers\"\n",
    "    FATALITIES_ALL = \"Fatalities_all\"\n",
    "    FATALITIES_PASSENGERS = \"Fatalities_passengers\"\n",
    "    GROUND = \"Ground:\"\n",
    "    REGISTRATION = \"Registration:\"\n",
    "    SUMMARY = \"Summary:\"\n",
    "    YEAR = \"Year\"\n",
    "    MONTH = \"Month\"\n",
    "    HOUR = \"Hour\"\n",
    "    ROUTES_N = \"Routes_Number\"\n",
    "    VECTOR = \"Vector\"\n",
    "    FATALITIES = \"Fatalities\"\n",
    "    SURVIVED = \"Survived\"\n",
    "    SURVIVED_PCT = \"Survived_pct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns = []\n",
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(\"?\", np.nan)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pct_missing_and_unique(col: str, alias: str = None) -> None:\n",
    "    alias = alias if alias else col\n",
    "    print(data[col].isna().value_counts())\n",
    "    print(f\"\\033[31m{alias} field has {data[col].isna().sum()/data.shape[0]:.2%} missing values\\033[0m\")\n",
    "    print(f\"\\033[32m{alias} field has {data[col].nunique()/data.shape[0]:.2%} unique values\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the variables that contain missing values\n",
    "vars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]\n",
    "\n",
    "# determine percentage of missing values (expressed as decimals)\n",
    "# and display the result ordered by % of missin data\n",
    "\n",
    "data[vars_with_na].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "data[vars_with_na].isnull().mean().sort_values(\n",
    "    ascending=False).plot.bar(figsize=(10, 4))\n",
    "plt.ylabel('Percentage of missing data')\n",
    "plt.axhline(y=0.80, color='r', linestyle='-')\n",
    "plt.axhline(y=0.15, color='g', linestyle='-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date: extract year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "get_pct_missing_and_unique(DataSchema.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year and month from date into separate columns\n",
    "data[DataSchema.YEAR] = pd.to_datetime(data[DataSchema.DATE]).dt.year\n",
    "data[DataSchema.MONTH] = pd.to_datetime(data[DataSchema.DATE]).dt.month\n",
    "redundant_columns.append(DataSchema.DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time: extract hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "get_pct_missing_and_unique(DataSchema.TIME, \"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour from time if time is not NaN else assign time to 25\n",
    "# TODO change hour 25 for eg. with most frequent value for same year, month, operator\n",
    "data[DataSchema.HOUR] = data[DataSchema.TIME].apply(\n",
    "    lambda x: dt.datetime.strptime(x, '%H:%M:%S').hour if x is not np.nan else 25)\n",
    "redundant_columns.append(DataSchema.TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location: check number of unique values, extract state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.LOCATION, \"Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[DataSchema.LOCATION].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations(text: str) -> Union[None, str]:    \n",
    "    if text is np.nan:\n",
    "        return text\n",
    "    locations = [] \n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"GPE\":\n",
    "            locations.append(ent.text)\n",
    "    return locations[-1] if locations else text\n",
    "\n",
    "data[DataSchema.LOCATION] = data[DataSchema.LOCATION].apply(lambda x: get_locations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[31mLocation field after parsing for country has {data[DataSchema.LOCATION].nunique()/data.shape[0]:.2%} unique values\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AC Type: check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.AC_TYPE, \"Aircraft Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[DataSchema.AC_TYPE].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO group Aircraft types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.OPERATOR, \"Operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[DataSchema.OPERATOR].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.OPERATOR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operatos_list(text: str) -> Union[None, list]:\n",
    "    if text is np.nan:\n",
    "        return text\n",
    "    if \"test\" in text.lower():\n",
    "        return text\n",
    "    operators = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            operators.append(ent.text)\n",
    "    return operators[-1] if operators else text\n",
    "\n",
    "# test\n",
    "# x = [np.nan, \"Military - U.S. Army Air Corps\", \"Test\", \"American Flyers Airline\", \"KLM Royal Dutch Airlines\", \"Air Canada\"]\n",
    "# for i in x:\n",
    "#     y = get_operatos_list(i)\n",
    "#     print(y)\n",
    "#     print(\"==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.copy()\n",
    "tmp[\"tmp\"] = tmp[DataSchema.OPERATOR].apply(lambda x: get_operatos_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"tmp\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[35mOperator field after organisation extraction has {tmp['tmp'].nunique()/data.shape[0]:.2%} unique values\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO consider to use tf-idf\n",
    "redundant_columns.append(DataSchema.OPERATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.ROUTE, \"Route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.ROUTE].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_locations(text: str) -> Union[None, str]:    \n",
    "    if text is np.nan:\n",
    "        return text\n",
    "    route = text.split(\" - \")    \n",
    "    for point in route:\n",
    "        doc = nlp(point)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"GPE\":\n",
    "                point = ent.text\n",
    "    flatten = list(itertools.chain(*[[j] if isinstance(j, str) else j for j in [i.split(\", \") for i in route]]))\n",
    "    return flatten\n",
    "\n",
    "\n",
    "# test\n",
    "# routes = [\n",
    "#     \"Sukhumi - Kutaisi\", \"Test flight\", np.nan, \"Bombing run\", \n",
    "#     \"Boston - NY - Washington DC - Jacksonville - Miami\", \"Lima - Pucallpa - Iquitos\"]\n",
    "# for _ in routes:\n",
    "#     x = get_multiple_locations(_)\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.ROUTE] = data[DataSchema.ROUTE].apply(lambda x: get_multiple_locations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.ROUTE].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.ROUTES_N] = data[DataSchema.ROUTE].apply(lambda x: 0 if x is np.nan else len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.ROUTES_N].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns.append(DataSchema.ROUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction or serial number / Line or fuselage number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.CN_LN, \"Construction/fuselage number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[DataSchema.CN_LN].notna()][DataSchema.CN_LN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns.append(DataSchema.CN_LN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.FLIGHT_N, \"Flight number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[DataSchema.FLIGHT_N].notna()][DataSchema.FLIGHT_N].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns.append(DataSchema.FLIGHT_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Military"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.IS_MILITARY, \"Is Military\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO consider to drop military samples\n",
    "redundant_columns.append(DataSchema.IS_MILITARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Military country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.MILITARY_COUNTRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns.append(DataSchema.MILITARY_COUNTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aboard All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.ABOARD_ALL, \"Aboard All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[DataSchema.ABOARD_ALL], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abroad Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.ABOARD_PASSENGERS, \"Abroad Passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fatalities All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.FATALITIES_ALL, \"Fatalities All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[DataSchema.FATALITIES_ALL], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fatalities Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.FATALITIES_PASSENGERS, \"Fatalities Passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.GROUND, \"Ground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.GROUND].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.GROUND].value_counts()\n",
    "# data[data[DataSchema.GROUND]==2750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO replace np.nan with mean which is 0 and change type to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatalities = [\n",
    "    DataSchema.ABOARD_ALL, DataSchema.ABOARD_PASSENGERS, \n",
    "    DataSchema.FATALITIES_ALL, DataSchema.FATALITIES_PASSENGERS, DataSchema.GROUND]\n",
    "data[fatalities].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pct_fatalities(row):\n",
    "    if row[DataSchema.ABOARD_ALL] is not np.nan and row[DataSchema.FATALITIES_ALL]is not np.nan and int(row[DataSchema.ABOARD_ALL])!=0:\n",
    "        return int(row[DataSchema.FATALITIES_ALL])/int(row[DataSchema.ABOARD_ALL])*100\n",
    "    return -1\n",
    "\n",
    "all = data.apply(lambda x: get_pct_fatalities(x), axis=1)\n",
    "\n",
    "def get_pct_fatalities_passwnger(row):\n",
    "    if row[DataSchema.ABOARD_ALL] is not np.nan and row[DataSchema.FATALITIES_PASSENGERS]is not np.nan and int(row[DataSchema.ABOARD_ALL])!=0:\n",
    "        return int(row[DataSchema.FATALITIES_PASSENGERS])/int(row[DataSchema.ABOARD_ALL])*100\n",
    "    return -1\n",
    "\n",
    "passengers = data.apply(lambda x: get_pct_fatalities(x), axis=1)\n",
    "\n",
    "def get_pct_fatalities_passwnger(row):\n",
    "    if row[DataSchema.ABOARD_ALL] is not np.nan and row[DataSchema.FATALITIES_PASSENGERS]is not np.nan and int(row[DataSchema.ABOARD_ALL])!=0:\n",
    "        return (int(row[DataSchema.FATALITIES_ALL])-int(row[DataSchema.FATALITIES_PASSENGERS]))/int(row[DataSchema.ABOARD_ALL])*100\n",
    "    return -1\n",
    "\n",
    "crew = data.apply(lambda x: get_pct_fatalities(x), axis=1)\n",
    "\n",
    "sns.histplot(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns.append(DataSchema.GROUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.REGISTRATION, \"Registration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.REGISTRATION].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns.append(DataSchema.REGISTRATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pct_missing_and_unique(DataSchema.SUMMARY, \"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[DataSchema.SUMMARY], inplace=True)\n",
    "data[DataSchema.SUMMARY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = api.info()\n",
    "for model_name, model_data in sorted(info[\"models\"].items()):\n",
    "    print(\"%s (%d records) %s...\" % (model_name, model_data.get(\"num_records\", -1), model_data[\"description\"][:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Castom data Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    filtered = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_stop:\n",
    "            continue\n",
    "        filtered.append(token.lemma_)\n",
    "    return filtered\n",
    "\n",
    "corpus = data[DataSchema.SUMMARY].apply(lambda text: get_preprocessed(text)).tolist()\n",
    "model_default = Word2Vec(sentences=corpus) # 100-dimentional vector by default\n",
    "model_vs100 = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model_vs50 = Word2Vec(sentences=corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot embadings in 2-dimensional spase\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    n_components = 2 # dimentions\n",
    "    # extract vocabulary from model and vectors in order to associate them in the graph\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "    #apply TSNE\n",
    "    tsne = TSNE(n_components=n_components, random_state=42)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    fig = go.Figure()\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode=\"markers\", text=labels)\n",
    "    fig.add_trace(trace)\n",
    "    fig.update_layout(title=\"Word2Vec - Visualisation embedding with TSNE\")\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "plot = plot_embeddings(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing (remove stop words, lemmitize)\n",
    "\n",
    "def text2vec(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    filtered = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_stop:\n",
    "            continue\n",
    "        filtered.append(token.lemma_)\n",
    "    return w2v.get_mean_vector(filtered, pre_normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sense check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2v.similarity(\"plane\",\"passanger\"))\n",
    "# print(w2v.most_similar(positive=[\"plane\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.VECTOR] = data[DataSchema.SUMMARY].apply(lambda text: text2vec(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.VECTOR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [DataSchema.ABOARD_ALL, DataSchema.FATALITIES_ALL, DataSchema.GROUND]:\n",
    "    data[col] = data[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.FATALITIES] = data[DataSchema.FATALITIES_ALL]+data[DataSchema.GROUND]\n",
    "data[DataSchema.SURVIVED] = data[DataSchema.ABOARD_ALL]-data[DataSchema.FATALITIES_ALL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.FATALITIES].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.SURVIVED].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.SURVIVED] = np.where(data[DataSchema.SURVIVED] > 0, 1, data[DataSchema.SURVIVED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.SURVIVED].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DataSchema.SURVIVED_PCT] = data[DataSchema.SURVIVED]/data[DataSchema.ABOARD_ALL]*100\n",
    "data[DataSchema.SURVIVED_PCT].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary vectors as features to predict survival percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[DataSchema.VECTOR].values,\n",
    "    data[DataSchema.SURVIVED],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data[DataSchema.SURVIVED]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d = np.stack(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train_2d.shape=}\")\n",
    "print(f\"{X_test_2d.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train_2d, y_train)\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('deploing_ml_models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f13d9f54e4f4e8bae757f010cebe67df66911819a91a6a446f98cddd5d9857d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
